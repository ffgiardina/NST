{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f3b985",
   "metadata": {},
   "source": [
    "# Neural style transfer\n",
    "\n",
    "Neural style transfer is a technique based on deep convolutional neural networks that applies the style of an image to a content image. Among many things, this allows one to take ones favorite pictures and reimagine them as a creation of one's favorite painter, or to, for example, reimagine how the Eiffel tower would look like if it were covered in leaves.\n",
    "\n",
    "This notebook makes the implementation of neural style transfer simple. By changing the content and style image paths in [Section 1](#1) one can quickly set up a new case with custom images and see the progress in the output of [Section 4](#4).\n",
    "\n",
    "This notebook is based on notes from the [Coursera](https://www.coursera.org/) deep learning specialization and uses the VGG-19 network. \n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "- [0. Imports](#0)\n",
    "- [1. Load content and style images](#1)\n",
    "- [2. Load pre-trained CNN](#2)\n",
    "- [3. Build the model](#3)\n",
    "- [4. Train the model](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301c447",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## 0. Imports\n",
    "This notebook requires `scipy`, `numpy`, `matplotlib`, and `tensorflow`. Some supporting functions are defined in the provided module `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e324468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57950d80",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1. Load content and style images\n",
    "\n",
    "The loaded content and style images are processed to have a 1:1 aspect ratio (using `make_square`), and of pixel size `img_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 400\n",
    "\n",
    "content_image = np.array(make_square(Image.open(\"images/escher_portrait.jpg\")).resize((img_size, img_size)))\n",
    "content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n",
    "\n",
    "style_image =  np.array(make_square(Image.open(\"images/escher.jpg\")).resize((img_size, img_size)))\n",
    "style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "imshow(content_image[0])\n",
    "ax1.title.set_text('Content image')\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "imshow(style_image[0])\n",
    "ax2.title.set_text('Style image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df0733",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Load pre-trained CNN\n",
    "The VGG-19 model and weights are not included in the github repository and need to be downloaded first and saved in the `pretrained-model` folder. [Download VGG-19 weights](\n",
    "https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(10)\n",
    "vgg = tf.keras.applications.VGG19(include_top=False,\n",
    "                                  input_shape=(img_size, img_size, 3),\n",
    "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "vgg.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b48e79",
   "metadata": {},
   "source": [
    "The layers of the loaded VGG-19 model are printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128581d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7097310",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Build the model\n",
    "\n",
    "#### 3.1 Define style cost weights\n",
    "Choose the layers in the VGG-19 network that will be used to compute the style cost. The tuple containts the name of the layer and the weight $\\lambda^{[l]}$ at which it enters the total style cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f662eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.05),\n",
    "    ('block2_conv1', .1),\n",
    "    ('block3_conv1', 0.15),\n",
    "    ('block4_conv1', 0.3),\n",
    "    ('block5_conv1', 0.3),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41253eb2",
   "metadata": {},
   "source": [
    "#### 3.2 Initialize generated image\n",
    "Add uniform random noise to the content image to initialize the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "noise = tf.random.uniform(tf.shape(generated_image), -0.25, 0.25)\n",
    "generated_image = tf.add(generated_image, noise)\n",
    "generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "imshow(generated_image.numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f33344",
   "metadata": {},
   "source": [
    "#### 3.3 Compute content and style layer outputs\n",
    "\n",
    "First, define the content layer and add it to to the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = [('block5_conv4', 1)]\n",
    "\n",
    "vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e207f",
   "metadata": {},
   "source": [
    "Compute the hidden layer activations for the content and style images and store them in `a_C` and `a_S`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
    "a_C = vgg_model_outputs(preprocessed_content)\n",
    "\n",
    "preprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n",
    "a_S = vgg_model_outputs(preprocessed_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dcf20",
   "metadata": {},
   "source": [
    "#### 3.4 Define the neural style transfer model\n",
    "\n",
    "Given the activations in the content layer for the content image $a^{(C)}$ and the generated image $a^{(G)}$, the content cost is defined as\n",
    "\n",
    "\n",
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $$\n",
    "\n",
    "where $n_H, n_W$ and $n_C$ are the height, width and number of channels of the hidden layer you have chosen.\n",
    "\n",
    "For the style cost, one needs to compute the Gram matrices $G^{(S)}_{(gram)}=a^{(S)} (a^{(S)})^T$ and $G^{(G)}_{(gram)}=a^{(G)} (a^{(G)})^T$ of the style and generated image, respectively. The style cost for layer $l$ can then be defined as\n",
    "\n",
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $$\n",
    "\n",
    "Taking the sum over all style layers and weighing the layer style cost by the previously defined weights in `STYLE_LAYERS`\n",
    "\n",
    "$$J_{style}(S,G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_{style}(S,G)\\tag{3}$$\n",
    "\n",
    "Finally, the content and style costs are added and weighted by the parameters $\\alpha$ and $\\beta$\n",
    "\n",
    "\n",
    "$$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)\\tag{4}$$\n",
    "\n",
    "Most of these functions are implemented in `utils.py`. What remains to be done is to define the training step in `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "alpha = 5\n",
    "beta = 100\n",
    "\n",
    "@tf.function()  # Use decorator to make train_step a tensorflow Function\n",
    "def train_step(generated_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        a_G = vgg_model_outputs(generated_image)\n",
    "        J_style = compute_style_cost(a_S, a_G, STYLE_LAYERS)\n",
    "        J_content = compute_content_cost(a_C, a_G)\n",
    "        J = total_cost(J_content, J_style, alpha, beta)\n",
    "\n",
    "    # compute gradient of tape\n",
    "    grad = tape.gradient(J, generated_image)\n",
    "    \n",
    "    # update generated_image using computed gradient\n",
    "    optimizer.apply_gradients([(grad, generated_image)])\n",
    "    \n",
    "    # assign new value to tf.Variable()\n",
    "    generated_image.assign(clip_0_1(generated_image)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b7c8d",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4. Train the model\n",
    "\n",
    "All that is left to do is run the model to train the generated image. Training can take quite long and with a learning rate of 0.001 around 10000 epochs need to be run. For quick results, reduce the learning rate and run for fewer epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7dfb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generated_image = tf.Variable(generated_image)\n",
    "\n",
    "# Show the generated image at some epochs\n",
    "epochs = 1001\n",
    "for i in range(epochs):\n",
    "    train_step(generated_image)\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Epoch {i} \")\n",
    "    if i % 20 == 0:\n",
    "        image = tensor_to_image(generated_image) \n",
    "        imshow(image)\n",
    "        image.save(f\"output/image_{i}.jpg\")\n",
    "        plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
